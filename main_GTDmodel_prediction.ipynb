{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'utils_GTDmodel' from 'utils_GTDmodel.py'>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "#import import_ipynb\n",
    "import utils_GTDmodel\n",
    "from joblib import dump, load\n",
    "import imp\n",
    "imp.reload(utils_GTDmodel)\n",
    "#from imblearn.combine import SMOTEENN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#DEFINE MAIN VARIABLES\n",
    "filename = 'gtd_70to94_0617dist.xlsx'\n",
    "var_name = '_70to94_03'\n",
    "filename_data = 'data'+var_name+'.npz'\n",
    "filename_one_hot = 'enc'+var_name+'.npz'\n",
    "#set weight to 'noWeight', 'balanced', 'computedWeight', 'resampling'\n",
    "weight = 'noWeight'\n",
    "weight_type = '_'+weight\n",
    "filename_tuning = 'tuning_cv'+weight_type+var_name+'.npz'\n",
    "model_type = 'modelSVC_rbf'\n",
    "C_best_type = '_C'\n",
    "gamma_best_type = 'g'\n",
    "features_dataset = ['iyear','imonth','iday','extended','crit1','crit2','crit3','doubtterr','country','region','attacktype1','success','weaptype1','targtype1']\n",
    "features_class = ['gname']\n",
    "n_folds = 5\n",
    "scoring = ['precision_weighted', 'recall_weighted', 'f1_weighted', 'precision_micro', 'recall_micro', 'f1_micro']\n",
    "Cs = [1, 10, 100, 1000]\n",
    "gammas = [0.001, 0.0001]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#LOADING AND SAVE DATASET AND TARGET SET ON FILE\n",
    "file_data = Path(filename_data)\n",
    "if not file_data.is_file():\n",
    "    saved_data_labels = utils_GTDmodel.dataset_creation(filename, filename_data, features_dataset, features_class)\n",
    "\n",
    "#LOADING DATASET AND TARGTET SET FROM COMPRESSED DATA FILE\n",
    "data = np.load(filename_data)\n",
    "dataset = data[data.files[1]]\n",
    "#dataset.shape (58099,14)\n",
    "target = data[data.files[0]]\n",
    "#target.shape (58099, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#SPLITTING DATASET INTO DATASET FOR TRAINING AND DATASET FOR PREDICTION\n",
    "\n",
    "#DATASET FOR PREDICTION: predict_data\n",
    "predict_data_index = np.where(target==\"Unknown\")[0]\n",
    "#predict_data_index.shape -> (18154,)\n",
    "predict_data = dataset[predict_data_index,:]\n",
    "#predict_data.shape -> (18154, 14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#DATASET FOR TRAINING: X_dataset\n",
    "X_dataset = np.delete(dataset, predict_data_index, 0)\n",
    "#X_dataset.shape -> (39945, 14)\n",
    "Y_target = np.delete(target, predict_data_index, 0)\n",
    "#Y_target.shape -> (39945, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#1.PREPROCESSING FEATURES: DELETE EXAMPLE WITH LESS THAN n_folds+2 EXAMPLES PER CLASS\n",
    "info_y_target, count_y_target = np.unique(Y_target, return_counts=True)\n",
    "#info_y_target.shape -> (1978, 1)\n",
    "#1978 classes in the original dataset\n",
    "\n",
    "for i in range(0, info_y_target.shape[0]):\n",
    "    if count_y_target[i] < (n_folds+2):\n",
    "        lessData_index = np.where(Y_target == info_y_target[i])[0]\n",
    "        X_dataset = np.delete(X_dataset, lessData_index, 0)\n",
    "        Y_target = np.delete(Y_target, lessData_index, 0)\n",
    "#X_dataset.shape-> (37119, 14)\n",
    "#Y_target.shape-> (37119, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new info label distribution:\n",
      "list sorted occurence labels\n",
      "[7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 14, 14, 14, 14, 14, 14, 14, 14, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 16, 16, 16, 16, 16, 16, 16, 16, 16, 17, 17, 17, 17, 17, 17, 17, 17, 17, 18, 18, 18, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 20, 20, 20, 20, 20, 20, 20, 20, 21, 21, 21, 21, 21, 21, 22, 22, 22, 22, 22, 23, 23, 23, 23, 23, 24, 24, 24, 24, 24, 24, 24, 24, 25, 25, 25, 25, 25, 25, 26, 26, 26, 26, 26, 27, 27, 28, 28, 28, 29, 29, 30, 30, 30, 31, 31, 31, 32, 33, 34, 34, 34, 34, 35, 36, 36, 36, 38, 39, 40, 40, 41, 41, 41, 43, 44, 44, 44, 45, 45, 46, 47, 47, 47, 48, 48, 49, 49, 49, 49, 50, 51, 52, 52, 53, 55, 55, 56, 56, 58, 58, 59, 59, 59, 59, 62, 62, 63, 64, 64, 67, 69, 73, 75, 75, 75, 76, 80, 82, 83, 84, 85, 86, 86, 89, 89, 90, 91, 93, 96, 97, 106, 106, 109, 111, 113, 116, 117, 120, 138, 141, 153, 157, 167, 170, 172, 174, 185, 192, 198, 204, 207, 207, 214, 215, 218, 221, 231, 239, 239, 244, 247, 282, 300, 306, 311, 366, 433, 460, 548, 551, 605, 632, 714, 822, 861, 880, 887, 895, 964, 1079, 1653, 2602, 3351, 4405]\n",
      "frequence labels\n",
      "[0.018858266655890514, 0.018858266655890514, 0.018858266655890514, 0.018858266655890514, 0.018858266655890514, 0.018858266655890514, 0.018858266655890514, 0.018858266655890514, 0.018858266655890514, 0.018858266655890514, 0.018858266655890514, 0.018858266655890514, 0.018858266655890514, 0.018858266655890514, 0.018858266655890514, 0.018858266655890514, 0.018858266655890514, 0.018858266655890514, 0.018858266655890514, 0.018858266655890514, 0.018858266655890514, 0.018858266655890514, 0.018858266655890514, 0.018858266655890514, 0.018858266655890514, 0.018858266655890514, 0.018858266655890514, 0.018858266655890514, 0.018858266655890514, 0.018858266655890514, 0.018858266655890514, 0.018858266655890514, 0.018858266655890514, 0.018858266655890514, 0.018858266655890514, 0.018858266655890514, 0.018858266655890514, 0.018858266655890514, 0.018858266655890514, 0.018858266655890514, 0.018858266655890514, 0.018858266655890514, 0.018858266655890514, 0.018858266655890514, 0.018858266655890514, 0.021552304749589159, 0.021552304749589159, 0.021552304749589159, 0.021552304749589159, 0.021552304749589159, 0.021552304749589159, 0.021552304749589159, 0.021552304749589159, 0.021552304749589159, 0.021552304749589159, 0.021552304749589159, 0.021552304749589159, 0.021552304749589159, 0.021552304749589159, 0.021552304749589159, 0.021552304749589159, 0.021552304749589159, 0.021552304749589159, 0.021552304749589159, 0.021552304749589159, 0.021552304749589159, 0.021552304749589159, 0.021552304749589159, 0.021552304749589159, 0.021552304749589159, 0.024246342843287805, 0.024246342843287805, 0.024246342843287805, 0.024246342843287805, 0.024246342843287805, 0.024246342843287805, 0.024246342843287805, 0.024246342843287805, 0.024246342843287805, 0.024246342843287805, 0.024246342843287805, 0.024246342843287805, 0.024246342843287805, 0.024246342843287805, 0.024246342843287805, 0.024246342843287805, 0.024246342843287805, 0.024246342843287805, 0.024246342843287805, 0.024246342843287805, 0.024246342843287805, 0.024246342843287805, 0.024246342843287805, 0.024246342843287805, 0.024246342843287805, 0.024246342843287805, 0.02694038093698645, 0.02694038093698645, 0.02694038093698645, 0.02694038093698645, 0.02694038093698645, 0.02694038093698645, 0.02694038093698645, 0.02694038093698645, 0.02694038093698645, 0.02694038093698645, 0.02694038093698645, 0.02694038093698645, 0.02694038093698645, 0.02694038093698645, 0.02694038093698645, 0.02694038093698645, 0.02694038093698645, 0.02694038093698645, 0.02694038093698645, 0.02694038093698645, 0.029634419030685092, 0.029634419030685092, 0.029634419030685092, 0.029634419030685092, 0.029634419030685092, 0.029634419030685092, 0.029634419030685092, 0.029634419030685092, 0.029634419030685092, 0.029634419030685092, 0.029634419030685092, 0.032328457124383737, 0.032328457124383737, 0.032328457124383737, 0.035022495218082379, 0.035022495218082379, 0.035022495218082379, 0.035022495218082379, 0.035022495218082379, 0.035022495218082379, 0.035022495218082379, 0.035022495218082379, 0.035022495218082379, 0.035022495218082379, 0.035022495218082379, 0.035022495218082379, 0.035022495218082379, 0.035022495218082379, 0.037716533311781028, 0.037716533311781028, 0.037716533311781028, 0.037716533311781028, 0.037716533311781028, 0.037716533311781028, 0.037716533311781028, 0.037716533311781028, 0.04041057140547967, 0.04041057140547967, 0.04041057140547967, 0.04041057140547967, 0.04041057140547967, 0.04041057140547967, 0.04041057140547967, 0.04041057140547967, 0.04041057140547967, 0.04041057140547967, 0.04041057140547967, 0.043104609499178319, 0.043104609499178319, 0.043104609499178319, 0.043104609499178319, 0.043104609499178319, 0.043104609499178319, 0.043104609499178319, 0.043104609499178319, 0.043104609499178319, 0.045798647592876961, 0.045798647592876961, 0.045798647592876961, 0.045798647592876961, 0.045798647592876961, 0.045798647592876961, 0.045798647592876961, 0.045798647592876961, 0.045798647592876961, 0.04849268568657561, 0.04849268568657561, 0.04849268568657561, 0.051186723780274251, 0.051186723780274251, 0.051186723780274251, 0.051186723780274251, 0.051186723780274251, 0.051186723780274251, 0.051186723780274251, 0.051186723780274251, 0.051186723780274251, 0.051186723780274251, 0.051186723780274251, 0.051186723780274251, 0.051186723780274251, 0.0538807618739729, 0.0538807618739729, 0.0538807618739729, 0.0538807618739729, 0.0538807618739729, 0.0538807618739729, 0.0538807618739729, 0.0538807618739729, 0.056574799967671542, 0.056574799967671542, 0.056574799967671542, 0.056574799967671542, 0.056574799967671542, 0.056574799967671542, 0.059268838061370184, 0.059268838061370184, 0.059268838061370184, 0.059268838061370184, 0.059268838061370184, 0.061962876155068826, 0.061962876155068826, 0.061962876155068826, 0.061962876155068826, 0.061962876155068826, 0.064656914248767475, 0.064656914248767475, 0.064656914248767475, 0.064656914248767475, 0.064656914248767475, 0.064656914248767475, 0.064656914248767475, 0.064656914248767475, 0.067350952342466117, 0.067350952342466117, 0.067350952342466117, 0.067350952342466117, 0.067350952342466117, 0.067350952342466117, 0.070044990436164759, 0.070044990436164759, 0.070044990436164759, 0.070044990436164759, 0.070044990436164759, 0.0727390285298634, 0.0727390285298634, 0.075433066623562056, 0.075433066623562056, 0.075433066623562056, 0.078127104717260698, 0.078127104717260698, 0.08082114281095934, 0.08082114281095934, 0.08082114281095934, 0.083515180904657996, 0.083515180904657996, 0.083515180904657996, 0.086209218998356638, 0.088903257092055279, 0.091597295185753921, 0.091597295185753921, 0.091597295185753921, 0.091597295185753921, 0.094291333279452577, 0.096985371373151219, 0.096985371373151219, 0.096985371373151219, 0.1023734475605485, 0.10506748565424714, 0.1077615237479458, 0.1077615237479458, 0.11045556184164444, 0.11045556184164444, 0.11045556184164444, 0.11584363802904173, 0.11853767612274037, 0.11853767612274037, 0.11853767612274037, 0.12123171421643901, 0.12123171421643901, 0.12392575231013765, 0.12661979040383631, 0.12661979040383631, 0.12661979040383631, 0.12931382849753495, 0.12931382849753495, 0.13200786659123359, 0.13200786659123359, 0.13200786659123359, 0.13200786659123359, 0.13470190468493223, 0.13739594277863088, 0.14008998087232952, 0.14008998087232952, 0.14278401896602816, 0.14817209515342547, 0.14817209515342547, 0.15086613324712411, 0.15086613324712411, 0.1562542094345214, 0.1562542094345214, 0.15894824752822004, 0.15894824752822004, 0.15894824752822004, 0.15894824752822004, 0.16703036180931599, 0.16703036180931599, 0.16972439990301463, 0.17241843799671328, 0.17241843799671328, 0.1805005522778092, 0.18588862846520651, 0.19666478084000108, 0.20205285702739836, 0.20205285702739836, 0.20205285702739836, 0.20474689512109701, 0.2155230474958916, 0.22091112368328888, 0.22360516177698753, 0.22629919987068617, 0.22899323796438481, 0.23168727605808345, 0.23168727605808345, 0.23976939033917938, 0.23976939033917938, 0.24246342843287802, 0.24515746652657666, 0.25054554271397395, 0.2586276569950699, 0.26132169508876857, 0.28556803793205632, 0.28556803793205632, 0.29365015221315233, 0.29903822840054961, 0.30442630458794689, 0.31250841886904279, 0.31520245696274146, 0.32328457124383736, 0.37177725693041302, 0.37985937121150892, 0.41218782833589263, 0.42296398071068719, 0.44990436164767372, 0.45798647592876962, 0.4633745521161669, 0.46876262830356419, 0.49839704733424928, 0.5172553139901398, 0.5334195425523317, 0.5495837711145235, 0.55766588539561945, 0.55766588539561945, 0.57652415205150997, 0.57921819014520859, 0.58730030442630465, 0.59538241870740061, 0.62232279964438697, 0.64387510439397611, 0.64387510439397611, 0.65734529486246929, 0.66542740914356535, 0.75971874242301785, 0.80821142810959345, 0.82437565667178525, 0.83784584714027854, 0.98601794229370399, 1.1665184945715132, 1.2392575231013767, 1.4763328753468574, 1.4844149896279535, 1.62989304668768, 1.7026320752175435, 1.9235431989008325, 2.2144993130202861, 2.3195667986745332, 2.3707535224548075, 2.389611789110698, 2.4111640938602874, 2.5970527223254933, 2.9068671031008377, 4.45324496888386, 7.0098871198038744, 9.0277216519841588, 11.867237802742531]\n",
      "max occurence: 4405\n",
      "min occurence: 7\n",
      "median: 19.0\n",
      "mode occurence: [45]\n",
      "the most frequent occurence [7]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0xe61eda0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjMAAAF3CAYAAABQc8olAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nJKnX/j/qVNxQ4iBxHgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11224ba8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#NEW INFO ABOUT THE LABELS DISTRUBUTION\n",
    "w_labels = []\n",
    "new_info_y_target, new_count_y_target = np.unique(Y_target, return_counts=True)\n",
    "tot_labels = new_info_y_target.shape[0]\n",
    "#tot_labels = 381\n",
    "tot_data = Y_target.shape[0]\n",
    "#tot_data = 37119\n",
    "utils_GTDmodel.print_plot_distribution(new_info_y_target, new_count_y_target, X_dataset.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature 0\n",
      "max: 1994\n",
      "min: 1970\n",
      "num_values: 24\n",
      "[ 0  0  0 ..., 23 23 23]\n",
      "[1970 1971 1972 1973 1974 1975 1976 1977 1978 1979 1980 1981 1982 1983 1984\n",
      " 1985 1986 1987 1988 1989 1990 1991 1992 1994]\n",
      "\n",
      "\n",
      "feature 1\n",
      "max: 12\n",
      "min: 0\n",
      "num_values: 13\n",
      "[ 0  1  1 ..., 12 12 12]\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12]\n",
      "\n",
      "\n",
      "feature 2\n",
      "max: 31\n",
      "min: 0\n",
      "num_values: 32\n",
      "[ 0  1  2 ..., 31 31 31]\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30 31]\n",
      "\n",
      "\n",
      "feature 3\n",
      "max: 1\n",
      "min: 0\n",
      "num_values: 2\n",
      "[0 0 0 ..., 0 0 0]\n",
      "[0 1]\n",
      "\n",
      "\n",
      "feature 4\n",
      "max: 1\n",
      "min: 0\n",
      "num_values: 2\n",
      "[1 1 1 ..., 1 1 1]\n",
      "[0 1]\n",
      "\n",
      "\n",
      "feature 5\n",
      "max: 1\n",
      "min: 0\n",
      "num_values: 2\n",
      "[1 1 1 ..., 1 1 1]\n",
      "[0 1]\n",
      "\n",
      "\n",
      "feature 6\n",
      "max: 1\n",
      "min: 0\n",
      "num_values: 2\n",
      "[1 1 1 ..., 1 1 1]\n",
      "[0 1]\n",
      "\n",
      "\n",
      "feature 7\n",
      "max: 1\n",
      "min: -9\n",
      "num_values: 3\n",
      "[1 1 1 ..., 1 0 1]\n",
      "[-9  0  1]\n",
      "\n",
      "\n",
      "feature 8\n",
      "max: 605\n",
      "min: 4\n",
      "num_values: 133\n",
      "[ 69 116 117 ..., 113  76 116]\n",
      "[  4   6   7   8  11  12  14  15  16  18  19  20  21  26  28  29  30  32\n",
      "  34  36  37  38  43  44  45  49  50  51  53  55  57  58  59  60  61  65\n",
      "  69  70  74  75  78  81  83  87  88  89  90  92  93  94  95  96  97  98\n",
      " 100 101 102 104 106 110 111 112 113 116 119 121 123 127 128 130 136 137\n",
      " 138 139 141 142 145 146 147 151 153 155 156 157 158 159 160 161 162 164\n",
      " 166 168 173 174 177 178 179 182 183 184 185 186 195 196 197 198 199 200\n",
      " 202 203 205 207 208 209 213 215 217 218 222 228 230 231 235 236 349 359\n",
      " 362 403 406 499 603 604 605]\n",
      "\n",
      "\n",
      "feature 9\n",
      "max: 12\n",
      "min: 1\n",
      "num_values: 12\n",
      "[0 0 2 ..., 9 1 0]\n",
      "[ 1  2  3  4  5  6  7  8  9 10 11 12]\n",
      "\n",
      "\n",
      "feature 10\n",
      "max: 9\n",
      "min: 1\n",
      "num_values: 9\n",
      "[5 1 0 ..., 2 1 1]\n",
      "[1 2 3 4 5 6 7 8 9]\n",
      "\n",
      "\n",
      "feature 11\n",
      "max: 1\n",
      "min: 0\n",
      "num_values: 2\n",
      "[1 1 0 ..., 1 1 1]\n",
      "[0 1]\n",
      "\n",
      "\n",
      "feature 12\n",
      "max: 13\n",
      "min: 2\n",
      "num_values: 10\n",
      "[9 1 1 ..., 9 1 1]\n",
      "[ 2  5  6  7  8  9 10 11 12 13]\n",
      "\n",
      "\n",
      "feature 13\n",
      "max: 22\n",
      "min: 1\n",
      "num_values: 22\n",
      "[6 2 2 ..., 0 0 4]\n",
      "[ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#2.PREPROCESSING FEATURES: FEATURES NORMALIZATION\n",
    "X_norm_dataset = utils_GTDmodel.normalize_features(X_dataset)\n",
    "#X_norm_dataset.shape-> (37119, 14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 24  13  32   2   2   2   2   3 133  12   9   2  10  22]\n",
      "[  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
      "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
      "  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n",
      "  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n",
      "  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107\n",
      " 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125\n",
      " 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143\n",
      " 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161\n",
      " 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179\n",
      " 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197\n",
      " 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215\n",
      " 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233\n",
      " 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251\n",
      " 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267]\n",
      "[  0  24  37  69  71  73  75  77  80 213 225 234 236 246 268]\n",
      "all\n",
      "encoding done\n",
      "(37119, 268)\n",
      "[array([ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  1.,  0.,  0.,  1.,  0.,  1.,  0.,  1.,  0.,\n",
      "        1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,\n",
      "        0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.])]\n"
     ]
    }
   ],
   "source": [
    "#3. PREPROCESSING FEATURES: ONE HOT ENCODING USING X_norm_dataset\n",
    "X_set = utils_GTDmodel.one_hot_encoding(filename_one_hot, X_norm_dataset)\n",
    "print('encoding done')\n",
    "print(X_set.shape)\n",
    "print(list(X_set[0:1].toarray()))\n",
    "\n",
    "#X_set.shape-> (37119, 268)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skip weight label computation because weight is equal to noWeight\n"
     ]
    }
   ],
   "source": [
    "#WEIGHT LABEL COMPUTATION WITH THRESHOLD\n",
    "if weight=='computedWeight':\n",
    "    delta_threshold = 0.0001\n",
    "    for i in range(len(new_count_y_target)):\n",
    "        p_i_threshold = new_count_y_target[i]/np.float(tot_data)\n",
    "        #print 'iter: '+str(i)+' p_i_threshold: '+str(p_i_threshold)\n",
    "        #new_array = np.array([utils.func_threshold((num_dati_curr_label/np.float(tot_data)), p_i_threshold, delta_threshold) for num_dati_curr_label in new_count_y_target])\n",
    "        #print list(new_array)\n",
    "        num_p_i = np.sum(np.array([utils_GTDmodel.func_threshold((num_dati_curr_label/np.float(tot_data)), p_i_threshold, delta_threshold) for num_dati_curr_label in new_count_y_target]))\n",
    "        w_labels.append(num_p_i / np.float(tot_labels))\n",
    "    print(w_labels)\n",
    "    print(new_count_y_target)\n",
    "    class_weight = dict( (new_info_y_target[l], w_labels[l]) for l in range(len(new_info_y_target)) )\n",
    "    print(class_weight)\n",
    "else:\n",
    "    print('skip weight label computation because weight is equal to '+weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skip imbalanced dataset resampling because weight is equal to noWeight\n"
     ]
    }
   ],
   "source": [
    "#RE-BALANCED DATSET USING IMBALANCED-LEARN\n",
    "if weight=='resampling':\n",
    "    X_resampled, y_resampled = utils_GTDmodel.imbalanced_resampling(X_set, Y_target)\n",
    "    X_set = X_resampled\n",
    "    Y_target = X_resampled\n",
    "else:\n",
    "    print('skip imbalanced dataset resampling because weight is equal to '+weight)\n",
    "#print('plotting sparse matrix dataset')\n",
    "#plt.figure(figsize=(14,12))\n",
    "#plt.spy(X_set, aspect='auto')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start parameters tuning\n",
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\beatrice.vincenzi\\Anaconda2\\lib\\site-packages\\sklearn\\model_selection\\_split.py:605: Warning: The least populated class in y has only 3 members, which is too few. The minimum number of members in any class cannot be less than n_splits=5.\n",
      "  % (min_groups, self.n_splits)), Warning)\n",
      "[Parallel(n_jobs=5)]: Done   3 tasks      | elapsed: 24.0min\n",
      "[Parallel(n_jobs=5)]: Done   8 tasks      | elapsed: 47.8min\n",
      "[Parallel(n_jobs=5)]: Done  15 tasks      | elapsed: 69.5min\n",
      "[Parallel(n_jobs=5)]: Done  22 tasks      | elapsed: 110.8min\n",
      "[Parallel(n_jobs=5)]: Done  31 tasks      | elapsed: 149.5min\n",
      "[Parallel(n_jobs=5)]: Done  36 out of  40 | elapsed: 168.7min remaining: 18.7min\n",
      "[Parallel(n_jobs=5)]: Done  40 out of  40 | elapsed: 171.3min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for RBF kernel with BALANCED weight classes:\n",
      "{'C': 1000, 'gamma': 0.001}\n"
     ]
    }
   ],
   "source": [
    "#TUNING HYPERPARAMETERS MODEL \n",
    "\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X_set, Y_target, test_size=0.20)\n",
    "Y_arr_target = [elem[0] for elem in Y_target]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_set, Y_arr_target, test_size=0.20)\n",
    "#X_train.shape -> (29695, 268)\n",
    "#y_train.shape -> (29695L, 1L)\n",
    "#X_test.shape -> (7424, 268)\n",
    "#y_test.shape -> (7424L, 1L)\n",
    "\n",
    "#BEST PARAMETERS\n",
    "#class_weight='noWeight': C = 1000; gamma = 0.001\n",
    "#array([ 0.54507493,  0.27964304,  0.66829433,  0.54527698,  0.75871359,\n",
    "#        0.66839535,  0.76922041,  0.75830948])\n",
    "#class_weight='balanced': C = 1000; gamma = 0.001; best_f1 = 0.69223775  \n",
    "#class_weight='computedWeight': C=1000; gamma=0.001; best_f1 = 0.60272773\n",
    "file_tuning = Path(filename_tuning)\n",
    "if not file_tuning.is_file():\n",
    "    if weight == 'computedWeight':\n",
    "        grid_search_res = utils_GTDmodel.svc_param_selection(filename_tuning, Cs, gammas, class_weight, n_folds, X_train, y_train)\n",
    "    else:\n",
    "        grid_search_res = utils_GTDmodel.svc_param_selection(filename_tuning, Cs, gammas, weight, n_folds, X_train, y_train)\n",
    "  \n",
    "    \n",
    "#PARAMETERS LOADING\n",
    "tuning_results = np.load(filename_tuning)\n",
    "f1_scores = tuning_results[tuning_results.files[0]]\n",
    "C = np.float(tuning_results[tuning_results.files[1]])\n",
    "gamma = np.float(tuning_results[tuning_results.files[2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibSVM]"
     ]
    }
   ],
   "source": [
    "#LEARNING MODEL\n",
    "filename_model = model_type+C_best_type+str(int(C))+gamma_best_type+str(gamma)+weight_type+var_name+'.pkl'\n",
    "file_model = Path(filename_model)\n",
    "if not file_model.is_file():\n",
    "    if weight == 'noWeight':\n",
    "        clf = SVC(kernel='rbf', C=C, gamma=gamma, verbose=10, decision_function_shape='ovr', random_state=42)\n",
    "    if weight == 'balanced':\n",
    "        clf = SVC(kernel='rbf', C=C, gamma=gamma, class_weight=weight, verbose=10, decision_function_shape='ovr', random_state=42)\n",
    "    if weight == 'computedWeight':\n",
    "        clf = SVC(kernel='rbf', C=C, gamma=gamma, class_weight=class_weight, verbose=10, decision_function_shape='ovr', random_state=42)\n",
    "    clf.fit(X_train, y_train)\n",
    "    #SAVE model\n",
    "    dump(clf, filename_model)\n",
    "else:\n",
    "    clf = load(filename_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\beatrice.vincenzi\\Anaconda2\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\beatrice.vincenzi\\Anaconda2\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "#TESTING MODEL\n",
    "y_predicted = clf.predict(X_test)\n",
    "\n",
    "#SCORES COMPUTATION\n",
    "f1_micro = f1_score(y_test, y_predicted, average='micro')\n",
    "recall_micro = recall_score(y_test, y_predicted, average='micro')\n",
    "precision_micro = precision_score(y_test, y_predicted, average='micro')\n",
    "# precision, recall, f1 and support for each label\n",
    "labels_target = np.unique(Y_target)\n",
    "scores_per_labels = precision_recall_fscore_support(y_test, y_predicted, average=None, labels=labels_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# SAVE SCORES INFO ON FILE\n",
    "filename_test_scores = 'scores_'+model_type+C_best_type+str(int(C))+gamma_best_type+str(gamma)+weight_type+var_name+'.npz'\n",
    "np.savez_compressed(filename_test_scores, f1_micro=f1_micro, precision_micro=precision_micro, recall_micro=recall_micro, labels_unique=labels_target, precision_labels=scores_per_labels[0], recall_labels=scores_per_labels[1], f1_labels=scores_per_labels[2], support_labels=scores_per_labels[3])\n",
    "#scores_tot = np.load(filename_test_scores)\n",
    "#scores_tot.files\n",
    "##['recall_labels', 'f1_labels', 'support_labels', 'precision_labels', 'precision_micro', 'labels_unique', 'recall_micro', 'f1_micro']\n",
    "#sl = scores_tot['support_labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([u'Dev Sol', u'Shining Path (SL)',\n",
       "       u'Basque Fatherland and Freedom (ETA)', ...,\n",
       "       u'Protestant extremists', u'Sikh Extremists',\n",
       "       u'Guerrilla Army of the Poor (EGP)'], \n",
       "      dtype='<U77')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scores_tot = np.load(filename_test_scores)\n",
    "#scores_tot.files\n",
    "##['recall_labels', 'f1_labels', 'support_labels', 'precision_labels', 'precision_micro', 'labels_unique', 'recall_micro', 'f1_micro']\n",
    "#sl = scores_tot['support_labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.54507493,  0.27964304,  0.66829433,  0.54527698,  0.75871359,\n",
       "        0.66839535,  0.76922041,  0.75830948])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
