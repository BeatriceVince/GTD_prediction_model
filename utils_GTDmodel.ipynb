{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn import preprocessing\n",
    "from joblib import dump\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from imblearn.combine import SMOTEENN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dataset_creation(filename, filename_data, features_dataset, features_class):\n",
    "    #LOADING DATASET FROM FILE\n",
    "    df = pd.read_excel(filename)\n",
    "    dataset = df.loc[:,features_dataset]\n",
    "    target = df.loc[:,features_class]\n",
    "    #SAVE DATASET AND TARGET SET ON FILE\n",
    "    np.savez_compressed(filename_data, dataset=dataset, target=target)\n",
    "    return_labels = ['dataset', 'target']\n",
    "    return np.array(return_labels, dtype=np.str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalize_features(dataset):\n",
    "    dataset_norm = np.copy(dataset)\n",
    "    #LABELS NORMALIZATION - LABELENCODER\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    for c in range(0,dataset_norm.T.shape[0]):\n",
    "        col = dataset_norm.T[c]\n",
    "        col_norm = le.fit_transform(col)\n",
    "        print('feature '+str(c))\n",
    "        print('max: '+str(np.max(col)))\n",
    "        print('min: '+str(np.min(col)))\n",
    "        print('num_values: '+str(le.classes_.size))\n",
    "        print(col_norm)\n",
    "        print(le.classes_)\n",
    "        print('\\n')\n",
    "        np.put(dataset_norm.T[c], range(0,dataset_norm.shape[0]), col_norm)\n",
    "    return dataset_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def one_hot_encoding(filename, dataset): \n",
    "    #ONE HOT ENCODING\n",
    "    enc = OneHotEncoder()\n",
    "    X_dataset = enc.fit_transform(dataset)\n",
    "    #SAVE the encoder to use later\n",
    "    dump(enc, filename)\n",
    "    print(enc.n_values_)\n",
    "    print(enc.active_features_)\n",
    "    print(enc.feature_indices_)\n",
    "    print(enc.categorical_features)\n",
    "    return X_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def svc_param_selection(filename_tuning, Cs, gamma, class_weight, n_folds, X_train, y_train):\n",
    "    print('start parameters tuning')\n",
    "    param_grid = {'C': Cs, 'gamma' : gammas}\n",
    "    grid_search = GridSearchCV(SVC(kernel='rbf', class_weight=class_weight), param_grid, cv=n_folds, scoring='f1_micro', verbose=10, n_jobs=5)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    print('Best parameters for RBF kernel with BALANCED weight classes:')\n",
    "    print(grid_search.best_params_) \n",
    "    #class_weight='balanced': C = 1000; gamma = 0.001; best_f1 = 0.69223775 \n",
    "    np.savez_compressed(filename_tuning, mean_f1_micro_test_score=mean_f1_micro_test_score, C=grid_search.best_params_['C'], gamma=grid_search.best_params_['gamma'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def imbalanced_resampling(X_set, Y_target):\n",
    "    #RE-BALANCED DATSET USING IMBALANCED-LEARN\n",
    "    #print('plotting sparse matrix dataset')\n",
    "    #plt.figure(figsize=(14,12))\n",
    "    #plt.spy(X_set, aspect='auto')\n",
    "    print('starting resample')\n",
    "    Y_arr_target = [elem[0] for elem in Y_target]\n",
    "    smote_enn = SMOTEENN(random_state=42)\n",
    "    smote_enn.fit(X_set, Y_arr_target)\n",
    "    X_resampled, y_resampled = smote_enn.sample(X_set, Y_arr_target)\n",
    "    #print(sorted(Counter(y_resampled).items()))\n",
    "    return X_resampled, y_resampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function to compute label weights \n",
    "def func_threshold(p_curr_label, p_i_threshold, delta_threshold):\n",
    "    if p_curr_label <= (p_i_threshold+delta_threshold) and p_curr_label >= (p_i_threshold-delta_threshold):\n",
    "        #print 'p_curr_label: '+str(p_curr_label)+' p_i_threshold: '+str(p_i_threshold)+' return 1'        \n",
    "        return 1\n",
    "    else:\n",
    "        #print 'p_curr_label: '+str(p_curr_label)+' p_i_threshold: '+str(p_i_threshold)+' return 0'\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def print_plot_distribution(new_info_y_target, new_count_y_target, tot_examples):\n",
    "    sorted_labels = np.sort(new_count_y_target)\n",
    "    index_sorted_labels = np.argsort(new_count_y_target)\n",
    "    perc_labels = (sorted_labels/float(tot_examples))*100\n",
    "    \n",
    "    print('new info label distribution:')\n",
    "    print('list sorted occurence labels')\n",
    "    print(str(list(sorted_labels)))\n",
    "    print('frequence labels')\n",
    "    print(str(list(perc_labels)))\n",
    "    print('max occurence: '+str(np.max(new_count_y_target)))\n",
    "    print('min occurence: '+str(np.min(new_count_y_target)))\n",
    "    #print('mean occurence: '+str(np.mean(new_count_y_target)))\n",
    "    print('median: '+str(np.median(new_count_y_target)))\n",
    "    mode = stats.mode(new_count_y_target)\n",
    "    print('mode occurence: '+str(mode[1]))\n",
    "    print('the most frequent occurence '+str(mode[0]))\n",
    "    #PLOT DISTRUBUTION\n",
    "    norm = []\n",
    "    for i in range(len(new_count_y_target)):\n",
    "        norm.append(list(new_count_y_target)[i]/(float(sum(list(new_count_y_target)))))\n",
    "    fig = plt.figure()\n",
    "    ind = np.arange(new_count_y_target.shape[0])                # the x locations for the groups\n",
    "    width = 0.35                      # the width of the bars\n",
    "    plt.bar(ind, norm, width, color='blue')\n",
    "    plt.xlim(-width,len(ind)+width)\n",
    "    plt.ylim(0,0.15)\n",
    "    plt.title(\"Target Labels Distribution\")\n",
    "    plt.xlabel(\"Labels\")\n",
    "    plt.ylabel(\"Normalized Frequency Labels\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
